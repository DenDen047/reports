@techreport{Gay,
abstract = {In this paper we present a framework to learn a model-free feedback controller for locomotion and balance control of a compliant quadruped robot walking on rough terrain. Having designed an open-loop gait encoded in a Central Pattern Generator (CPG), we use a neural network to represent sensory feedback inside the CPG dynamics. This neural network accepts sensory inputs from a gyroscope or a camera, and its weights are learned using Particle Swarm Optimization (unsupervised learning). We show with a simulated compliant quadruped robot that our controller can perform significantly better than the open-loop one on slopes and randomized height maps.},
author = {Gay, S{\'{e}}bastien and Santos-Victor, Jos{\'{e}} and Ijspeert, Auke},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Gay, Santos-Victor, Ijspeert - Unknown - Learning Robot Gait Stability using Neural Networks as Sensory Feedback Function for Central Pa.pdf:pdf},
title = {{Learning Robot Gait Stability using Neural Networks as Sensory Feedback Function for Central Pattern Generators}},
url = {https://infoscience.epfl.ch/record/187784/files/paper.pdf}
}
@techreport{Tan,
abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.10332v2},
author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent and Brain, Google and Deepmind, Google},
eprint = {arXiv:1804.10332v2},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Tan et al. - Unknown - Sim-to-Real Learning Agile Locomotion For Quadruped Robots.pdf:pdf},
title = {{Sim-to-Real: Learning Agile Locomotion For Quadruped Robots}},
url = {https://arxiv.org/pdf/1804.10332.pdf}
}
@article{Srouji,
abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlin-ear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward , and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI Mu-JoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomo-tion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.08311v1},
author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
eprint = {arXiv:1802.08311v1},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Srouji, Zhang, Salakhutdinov - Unknown - Structured Control Nets for Deep Reinforcement Learning.pdf:pdf},
title = {{Structured Control Nets for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1802.08311.pdf}
}
@article{Levine,
abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These tra-jectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.},
author = {Levine, Sergey and Abbeel, Pieter},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Levine, Abbeel - Unknown - Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics.pdf:pdf},
title = {{Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics}},
url = {https://people.eecs.berkeley.edu/{~}svlevine/papers/mfcgps.pdf}
}
@article{Lillicrap,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies "end-to-end": directly from raw pixel inputs .},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971v5},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {arXiv:1509.02971v5},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Lillicrap et al. - Unknown - CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING.pdf:pdf},
title = {{CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING}},
url = {https://goo.gl/J4PIAz}
}
@article{Kendall,
abstract = {We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.00412v1},
author = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Amar, Alex Bewley and Wayve, Shah},
eprint = {arXiv:1807.00412v1},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Kendall et al. - Unknown - Learning to Drive in a Day.pdf:pdf},
keywords = {Autonomous Vehicles,Deep Reinforcement Learning},
title = {{Learning to Drive in a Day}},
url = {https://arxiv.org/pdf/1807.00412.pdf}
}
@article{Zhu,
abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment. The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.05143v1},
author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
eprint = {arXiv:1609.05143v1},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.pdf:pdf},
title = {{Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning *}},
url = {https://youtu.be/SmBxMDiOrvs.}
}
@article{James,
abstract = {Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.03759v2},
author = {James, Stephen and Johns, Edward},
eprint = {arXiv:1609.03759v2},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/James, Johns - Unknown - 3D Simulation for Robot Arm Control with Deep Q-Learning(2).pdf:pdf},
title = {{3D Simulation for Robot Arm Control with Deep Q-Learning}},
url = {https://arxiv.org/pdf/1609.03759.pdf}
}
@article{Levinea,
abstract = {Autonomous learning of object manipulation skills can enable robots to acquire rich behavioral repertoires that scale to the variety of objects found in the real world. However, current motion skill learning methods typically restrict the behavior to a compact, low-dimensional representation, limiting its expressiveness and generality. In this paper, we extend a recently developed policy search method [1] and use it to learn a range of dynamic manipulation behaviors with highly general policy representations, without using known models or example demonstrations. Our approach learns a set of trajectories for the desired motion skill by using iteratively refitted time-varying linear models, and then unifies these trajectories into a single control policy that can generalize to new situations. To enable this method to run on a real robot, we introduce several improvements that reduce the sample count and automate parameter selection. We show that our method can acquire fast, fluent behaviors after only minutes of interaction time, and can learn robust controllers for complex tasks, including putting together a toy airplane, stacking tight-fitting lego blocks, placing wooden rings onto tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps onto bottles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.05611v2},
author = {Levine, Sergey and Wagener, Nolan and Abbeel, Pieter},
eprint = {arXiv:1501.05611v2},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Levine, Wagener, Abbeel - Unknown - Learning Contact-Rich Manipulation Skills with Guided Policy Search.pdf:pdf},
title = {{Learning Contact-Rich Manipulation Skills with Guided Policy Search}},
url = {https://arxiv.org/pdf/1501.05611.pdf}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00702v5},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
eprint = {arXiv:1504.00702v5},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies(2).pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {https://arxiv.org/pdf/1504.00702.pdf},
volume = {17},
year = {2016}
}
@article{Sadeghi,
abstract = {Environment feedback t = 0 t = 1 t = 2 t = 3 t = 4 t = H {\ldots} Action Collision with wall Collision with Furniture No Collision Different Platforms time Training entirely in simulation Test in real world Fig. 1. We propose the Collision Avoidance via Deep Reinforcement Learning algorithm for indoor flight which is entirely trained in a simulated CAD environment. Left: CAD 2 RL uses single image inputs from a monocular camera, is exclusively trained in simulation, and does not see any real images at training time. Training is performed using a Monte Carlo policy evaluation method, which performs rollouts for multiple actions from each initial state and trains a deep network to predict long-horizon collision probabilities of each action. Right: CAD 2 RL generalizes to real indoor flight. Abstract-Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD 2 RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.04201v4},
author = {Sadeghi, Fereshteh and Levine, Sergey},
eprint = {arXiv:1611.04201v4},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Sadeghi, Levine - Unknown - CAD 2 RL Real Single-Image Flight Without a Single Real Image.pdf:pdf},
title = {{CAD 2 RL: Real Single-Image Flight Without a Single Real Image}},
url = {https://youtu.be/nXBWmzFrj5s}
}
@article{Bousmalis,
abstract = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.07857v2},
author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
eprint = {arXiv:1709.07857v2},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Bousmalis et al. - Unknown - Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping.pdf:pdf},
title = {{Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping}},
url = {https://arxiv.org/pdf/1709.07857.pdf}
}
@article{Tassa2018,
abstract = {The DeepMind Control Suite is a set of continuous control tasks with a stan-dardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and mod-ify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at github.com/deepmind/dm{\_}control. A video summary of all tasks is available at youtu.be/rAai4QzcYbs.},
author = {Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and De, Diego and Casas, Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and Lillicrap, Timothy and Riedmiller, Martin},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Tassa et al. - 2018 - DeepMind Control Suite.pdf:pdf},
title = {{DeepMind Control Suite}},
url = {https://arxiv.org/pdf/1801.00690.pdf},
year = {2018}
}
@article{Long,
abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. How-ever, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representa-tions of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statisti-cal guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empiri-cal evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Long et al. - Unknown - Learning Transferable Features with Deep Adaptation Networks.pdf:pdf},
title = {{Learning Transferable Features with Deep Adaptation Networks}},
url = {http://proceedings.mlr.press/v37/long15.pdf}
}
@article{Mnih,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforce-ment learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray and Com, Korayk@google and Deepmind, Google},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - Unknown - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1602.01783.pdf}
}
@article{Liu2017,
author = {Liu, Libin and Hodgins, Jessica and Liu, Libin and Hodgins, Jessica},
doi = {10.1145/3083723},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2017 - Learning to Schedule Control Fragments for Physics-Based Characters Using Deep Q-Learning.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {Human simulation,deep Q-learning,motion control},
month = {jun},
number = {3},
pages = {1--14},
publisher = {ACM},
title = {{Learning to Schedule Control Fragments for Physics-Based Characters Using Deep Q-Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3087678.3083723},
volume = {36},
year = {2017}
}
@article{Dzeladini2014,
abstract = {Although the concept of central pattern generators (CPGs) controlling locomotion in vertebrates is widely accepted, the presence of specialized CPGs in human locomotion is still a matter of debate. An interesting numerical model developed in the 90s' demonstrated the important role CPGs could play in human locomotion, both in terms of stability against perturbations, and in terms of speed control. Recently, a reflex-based neuro-musculo-skeletal model has been proposed, showing a level of stability to perturbations similar to the previous model, without any CPG components. Although exhibiting striking similarities with human gaits, the lack of CPG makes the control of speed/step length in the model difficult. In this paper, we hypothesize that a CPG component will offer a meaningful way of controlling the locomotion speed. After introducing the CPG component in the reflex model, and taking advantage of the resulting properties, a simple model for gait modulation is presented.The results highlight the advantages that a feedforward component can have in terms of gait modulation.},
author = {Dzeladini, Florin and van den Kieboom, Jesse and Ijspeert, Auke},
doi = {10.3389/fnhum.2014.00371},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Dzeladini, van den Kieboom, Ijspeert - 2014 - The contribution of a central pattern generator in a reflex-based neuromuscular model.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {3FBL,Central pattern generators (CPG),Feedback based locomotion,Feedforward and feedback based locomotion,Feedforward based locomotion,Musculoskeletal System,central pattern generator,dynamical system model,feedback control,feedback predictor,feedforward,human locomotion,musculoskeletal modeling,neuromuscular model},
month = {jun},
pages = {371},
publisher = {Frontiers},
title = {{The contribution of a central pattern generator in a reflex-based neuromuscular model}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2014.00371/abstract},
volume = {8},
year = {2014}
}
@article{Ha2015,
abstract = {—We present a novel computational approach to de-signing robotic devices from high-level motion specifications. Our computational system uses a library of modular components— actuators, mounting brackets, and connectors—to define the space of possible robot designs. The process of creating a new robot begins with a set of input trajectories that specify how its end effectors and/or body should move. By searching through the combinatorial set of possible arrangements of modular components, our method generates a functional, as-simple-as-possible robotic device that is capable of tracking the input motion trajectories. To significantly improve the efficiency of this discrete optimization process, we propose a novel heuristic that guides the search for appropriate designs. Briefly, our heuristic function estimates how much an intermediate robot design needs to change before it becomes able to execute the target motion trajectories. We demonstrate the effectiveness of our computational design method by automatically creating a variety of robotic manipulators and legged robots. To generate these results we define our own robotic kit that includes off-the-shelf actuators and 3D printable connectors. We validate our results by fabricating two robotic devices designed with our method.},
author = {Ha, Sehoon and Coros, Stelian and Alspach, Alexander and Bern, James M and Kim, Joohyung and Yamane, Katsu},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Ha et al. - 2015 - Computational Design of Robotic Devices from High-Level Motion Specifications.pdf:pdf},
journal = {IEEE TRANSACTIONS ON ROBOTICS},
keywords = {Index Terms—Mechanism Design,Kinematics,Legged Robots,Manipula-tion Planning},
number = {8},
title = {{Computational Design of Robotic Devices from High-Level Motion Specifications}},
url = {https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20180629153607/Computational-Design-of-Robotic-Devices-from-High-Level-Motion-Specifications-Paper.pdf},
volume = {14},
year = {2015}
}
@article{Chen,
author = {Chen, Boyuan},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Chen - Unknown - Reinforcement Learning in Robotics.pdf:pdf},
title = {{Reinforcement Learning in Robotics}},
url = {http://www.cs.columbia.edu/{~}bchen/}
}
@article{Ha,
abstract = {— In this paper, we present an automated learning environment for developing control policies directly on the hardware of a modular legged robot. This environment fa-cilitates the reinforcement learning process by computing the rewards using a vision-based tracking system and relocating the robot to the initial position using a resetting mechanism. We employ two state-of-the-art deep reinforcement learning (DRL) algorithms, Trust Region Policy Optimization (TRPO) and Deep Deterministic Policy Gradient (DDPG), to train neural network policies for simple rowing and crawling motions. Using the developed environment, we demonstrate both learning algorithms can effectively learn policies for simple locomotion skills on highly stochastic hardware and environments. We further expedite learning by transferring policies learned on a single legged configuration to multi-legged ones.},
annote = {This paper presented an automated learning environment using a vision-based tracking system and relocating the robot. to the initial position using a resetting mechanism and showed two state-of-the-art learning methods.},
author = {Ha, Sehoon and Kim, Joohyung and Yamane, Katsu},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Ha, Kim, Yamane - Unknown - Automated Deep Reinforcement Learning Environment for Hardware of a Modular Legged Robot.pdf:pdf},
keywords = {summarized},
mendeley-tags = {summarized},
title = {{Automated Deep Reinforcement Learning Environment for Hardware of a Modular Legged Robot}},
url = {https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20180625141830/Automated-Deep-Reinforcement-Learning-Environment-for-Hardware-of-a-Modular-Legged-Robot-Paper.pdf}
}
@article{Zamorab,
author = {Zamora, Iker and {Gonzalez Lopez}, Nestor and Vilches, V{\'{i}}ctor Mayoral and {Hern{\'{a}}ndez Cordero}, Alejandro and Robotics, Erle},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Zamora et al. - Unknown - Extending the OpenAI Gym for robotics a toolkit for reinforcement learning using ROS and Gazebo(2).pdf:pdf},
title = {{Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo}},
url = {https://arxiv.org/pdf/1608.05742.pdf}
}
@article{Sadeghia,
abstract = {Query object Initial Reaching Train in Recurrent Control Test IIWA Robotic Unseen object Initial Reaching Figure 1. Illustration of our learned recurrent visual servoing controller . Training is performed in simulation (left) to reach varied objects from various viewpoints . The recurrent controller learns to implicitly calibrate the image - space motion of the arm with respect to the actions issued in the unknown coordinate frame of the robot . The model is then transferred to the real world by adapting the visual features , and can reach previously unseen objects from novel viewpoints (right) . Abstract Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints . In robotics , this ability is referred to as visual servo - ing : moving a tool or end - point to a desired location using primarily visual feedback . In this paper , we pro - pose learning viewpoint invariant visual servoing skills in a robot manipulation task . We train a deep recur - rent controller that can automatically determine which actions move the end - effector of a robotic arm to a de - sired object . This problem is fundamentally ambigu - ous : under severe variation in viewpoint , it may be impossible to determine the actions in a single feed - forward operation . Instead , our visual servoing ap - proach uses its memory of past movements to understand how the actions affect the robot motion from the cur - rent viewpoint , correcting mistakes and gradually mov - ing closer to the target . This ability is in stark con - trast to previous visual servoing methods , which assume known dynamics or require a calibration phase . We learn our recurrent controller using simulated data , syn - thetic demonstrations and reinforcement learning . We then describe how the resulting model can be trans - ferred to a real - world robot by disentangling percep - tion from control and only adapting the visual lay - ers . The adapted model can servo to previously un - seen objects from novel viewpoints on a real - world Kuka IIWA robotic arm . For supplementary videos , see : https : / / www . youtube . com / watch ? v=oLgM2Bnb7fo},
author = {Sadeghi, Fereshteh and Toshev, Alexander and Jang, Eric and Levine, Sergey},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Sadeghi et al. - Unknown - Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control.pdf:pdf},
title = {{Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Sadeghi{\_}Sim2Real{\_}Viewpoint{\_}Invariant{\_}CVPR{\_}2018{\_}paper.pdf}
}
@article{Zhang,
abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuf-fle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior perfor-mance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8{\%}) than recent MobileNet [12] on Ima-geNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ∼13× actual speedup over AlexNet while main-taining comparable accuracy.},
annote = {ShuffleNet{\~{}}$\backslash$cite{\{}Zhang{\}} using pointwise group convolution and channel shuffle is a computation-efficient Convolutional Neural Network(CNN) architecture for mobile devices.},
author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - ShuffleNet An Extremely Efficient Convolutional Neural Network for Mobile Devices.pdf:pdf},
title = {{ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}},
url = {https://arxiv.org/pdf/1707.01083.pdf}
}
@article{Rusu,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Rusu et al. - Unknown - Progressive Neural Networks.pdf:pdf},
title = {{Progressive Neural Networks}},
url = {https://arxiv.org/pdf/1606.04671.pdf}
}
@article{,
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 複数のゲームにおける continual learning.pdf:pdf},
title = {複数のゲームにおける continual learning},
url = {https://wba-initiative.org/wp-content/uploads/2015/05/20161008-hack2-noguchi.pdf}
}
@article{Rusua,
abstract = {Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.},
annote = {$\backslash$cite{\{}Rusua{\}} demonstrated the task learning from raw visual input on a fully actuated robot manipulator in about 4 hours.
This work bridged the reality gap and transfer learned policies
from simulation to the real world using Progressive Neural Networks{\~{}}$\backslash$cite{\{}Rusu{\}}.},
author = {Rusu, Andrei A and Ve{\v{c}}er{\'{i}}k, Mel and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Rusu et al. - Unknown - Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:pdf},
keywords = {CoRL,Robot learning,progressive networks,sim-to-real,transfer},
title = {{Sim-to-Real Robot Learning from Pixels with Progressive Nets}},
url = {https://arxiv.org/pdf/1610.04286.pdf}
}
@article{Zamoraa,
author = {Zamora, Iker and {Gonzalez Lopez}, Nestor and Vilches, V{\'{i}}ctor Mayoral and {Hern{\'{a}}ndez Cordero}, Alejandro and Robotics, Erle},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Zamora et al. - Unknown - Extending the OpenAI Gym for robotics a toolkit for reinforcement learning using ROS and Gazebo.pdf:pdf},
title = {{Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo}},
url = {http://erlerobotics.com/whitepaper/robot{\_}gym.pdf}
}
@article{,
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 2足歩行運動を生成する神経系構造の自律的獲得.pdf:pdf},
title = {2足歩行運動を生成する神経系構造の自律的獲得},
url = {https://www.jstage.jst.go.jp/article/kikaic1979/64/625/64{\_}625{\_}3541/{\_}pdf/-char/ja}
}
@article{,
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 生物規範型歩行ロボット制御.pdf:pdf},
title = {生物規範型歩行ロボット制御},
url = {https://www.jstage.jst.go.jp/article/sicejl1962/40/6/40{\_}6{\_}441/{\_}pdf/-char/ja}
}
@article{Cully2015,
abstract = {An intelligent trial-and-error learning algorithm is presented that allows robots to adapt in minutes to compensate for a wide variety of types of damage.},
author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
doi = {10.1038/nature14422},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Cully et al. - 2015 - Robots that can adapt like animals.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {may},
number = {7553},
pages = {503--507},
publisher = {Nature Publishing Group},
title = {{Robots that can adapt like animals}},
url = {http://www.nature.com/articles/nature14422},
volume = {521},
year = {2015}
}
@article{Eysenbach,
abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN (" Diversity is All You Need "), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Eysenbach et al. - Unknown - Diversity is All You Need Learning Skills without a Reward Function.pdf:pdf},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
url = {https://arxiv.org/pdf/1802.06070.pdf}
}
@article{Gupta,
abstract = {Meta-learning is a powerful tool that builds on multi-task learning to learn how to quickly adapt a model to new tasks. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by meta-learning prior tasks. The performance of meta-learning algorithms critically depends on the tasks available for meta-training: in the same way that supervised learning algorithms generalize best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We describe a general recipe for unsuper-vised meta-reinforcement learning, and describe an effective instantiation of this approach based on a recently proposed unsupervised exploration technique and model-agnostic meta-learning. We also discuss practical and conceptual consid-erations for developing unsupervised meta-learning methods. Our experimental results demonstrate that unsupervised meta-reinforcement learning effectively ac-quires accelerated reinforcement learning procedures without the need for manual task design, significantly exceeds the performance of learning from scratch, and even matches performance of meta-learning methods that use hand-specified task distributions.},
author = {Gupta, Abhishek and Eysenbach, Benjamin and Finn, Chelsea and Levine, Sergey},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Gupta et al. - 2018 - Unsupervised Meta-Learning for Reinforcement Learning.pdf:pdf},
title = {{Unsupervised Meta-Learning for Reinforcement Learning}},
url = {https://arxiv.org/pdf/1806.04640.pdf}
}
@article{Abbeel,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a re-ward function, but where instead we can ob-serve an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as try-ing to maximize a reward function that is ex-pressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our al-gorithm is based on using " inverse reinforce-ment learning " to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy out-put by the algorithm will attain performance close to that of the expert, where here per-formance is measured with respect to the ex-pert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Abbeel, Ng - Unknown - Apprenticeship Learning via Inverse Reinforcement Learning.pdf:pdf},
title = {{Apprenticeship Learning via Inverse Reinforcement Learning}},
url = {http://delivery.acm.org/10.1145/1020000/1015430/p335-abbeel.pdf?ip=133.51.28.16{\&}id=1015430{\&}acc=ACTIVE SERVICE{\&}key=D2341B890AD12BFE.5A1FE8BDE322CB75.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1528625734{\_}383e4b71b3c0462d23730daa2603b63f}
}
@article{Zamora,
author = {Zamora, Iker and {Gonzalez Lopez}, Nestor and Vilches, V{\'{i}}ctor Mayoral and {Hern{\'{a}}ndez Cordero}, Alejandro and Robotics, Erle},
file = {:Users/ikuta/Library/Application Support/Mendeley Desktop/Downloaded/Zamora et al. - Unknown - Extending the OpenAI Gym for robotics a toolkit for reinforcement learning using ROS and Gazebo.pdf:pdf},
title = {{Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo}},
url = {http://erlerobotics.com/whitepaper/robot{\_}gym.pdf}
}
